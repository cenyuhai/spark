# Default system properties included when running spark-submit.
#
# This is useful for setting default environmental settings.

spark.master                      yarn
spark.submit.deployMode           client

#event log
spark.eventLog.enabled            true
spark.eventLog.compress           false
spark.eventLog.dir                hdfs://mycluster-tj/user/xiaoju/spark/historylog
spark.history.fs.logDirectory     hdfs://mycluster-tj/user/xiaoju/spark/historylog


### yarn ###
spark.yarn.historyServer.address	bigdata-hdp-spark00.xg01:18080
spark.yarn.jars=hdfs://mycluster-tj/user/xiaoju/spark/jars-2.1-100/*


### max resource allocate #################
spark.driver.memory              4G
spark.executor.memory            10G
spark.executor.cores             6


## memory leak risk #################
#spark.ui.enabled true
spark.ui.retainedJobs 10
spark.ui.retainedStages 50
spark.ui.retainedTasks 50000
spark.ui.dagGraph.retainedRootRDDs 2
spark.history.retainedApplications 5
spark.deploy.retainedApplications 10
spark.deploy.retainedDrivers  10
spark.streaming.ui.retainedBatches 50
spark.sql.thriftserver.ui.retainedSessions 10
spark.sql.thriftserver.ui.retainedStatements 10
spark.sql.ui.retainedExecutions 10
spark.ui.retainedDeadExecutors 10
spark.ui.timeline.tasks.maximum 50
spark.ui.timeline.executors.maximum 50
spark.history.ui.maxApplications 5000

#sql hive
spark.sql.orc.filterPushdown true
spark.sql.shuffle.partitions 1000
spark.sql.autoBroadcastJoinThreshold 100000000
#spark.sql.hive.verifyPartitionPath true
spark.sql.hive.metastorePartitionPruning true
spark.sql.parquet.binaryAsString true
spark.sql.hive.convertMetastoreParquet false
spark.sql.specializeSingleDistinctAggPlanning true
spark.sql.adaptive.enabled true


####  debug classpath java #######
spark.driver.extraJavaOptions -XX:NewRatio=8 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+UseCompressedOops -XX:+UseStringDeduplication
spark.executor.extraJavaOptions -XX:NewRatio=8 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+UseCompressedOops -XX:+UseStringDeduplication

#optimizer
spark.port.maxRetries 40
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer 32
spark.kryoserializer.buffer.max 1024
spark.driver.maxResultSize 0
spark.io.compression.codec lz4
spark.hadoop.validateOutputSpecs false
spark.history.fs.cleaner.enabled true
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2
spark.hadoop.dfs.client.hedged.read.threadpool.size 3
spark.hadoop.dfs.client.hedged.read.threshold.millis 1000
spark.blacklist.enabled false
spark.scheduler.listenerbus.eventqueue.size 100000


#locality
spark.locality.wait.process 1ms
spark.locality.wait.node 3ms
spark.locality.wait.rack 30ms


#dynamic allocation
spark.dynamicAllocation.enabled true
spark.dynamicAllocation.minExecutors 1
spark.dynamicAllocation.maxExecutors 100
spark.shuffle.service.enabled true

#timeout
spark.rpc.lookupTimeout 120s
spark.rpc.askTimeout 120s
spark.network.timeout 600s

#memory management
#spark.shuffle.io.preferDirectBufs false
spark.unifiedMemory.useStaticStorageRegion true
#spark.yarn.executor.memoryOverhead.factor 0.2
spark.yarn.executor.memoryOverhead 2048

#spark.scheduler.allocation.file=/data/hadoop/spark/conf/fairscheduler.xml
#spark.scheduler.pool=default
#spark.scheduler.mode=FAIR


#metrics
spark.metrics.conf.driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource
spark.metrics.conf.executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource

